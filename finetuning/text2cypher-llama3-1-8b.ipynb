{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3678ad53936b423c945e98a4042a54f4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b9fc0bdb6fc44314a30022fedd40736d","IPY_MODEL_58aed0adf72240968b9ecbb054aa4e07","IPY_MODEL_49ccde7e04ba416e819c813a66cb4fed"],"layout":"IPY_MODEL_38db07b7dea248a19e10ecb5bf5449f0"}},"b9fc0bdb6fc44314a30022fedd40736d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_61d5724f9d804a3f833bee5b145b93c1","placeholder":"â€‹","style":"IPY_MODEL_39578213db5d4b3ba263d7af8015a077","value":"Mapâ€‡(num_proc=2):â€‡100%"}},"58aed0adf72240968b9ecbb054aa4e07":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_153d695617d24716b2355f310573904b","max":3703,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1f63336739bc4611ac3253bf2ed25af5","value":3703}},"49ccde7e04ba416e819c813a66cb4fed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d00300fae1442379e9308ac8411e53a","placeholder":"â€‹","style":"IPY_MODEL_1ae1137c21264f51a9808c25434dc7b2","value":"â€‡3703/3703â€‡[00:10&lt;00:00,â€‡526.03â€‡examples/s]"}},"38db07b7dea248a19e10ecb5bf5449f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61d5724f9d804a3f833bee5b145b93c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39578213db5d4b3ba263d7af8015a077":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"153d695617d24716b2355f310573904b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f63336739bc4611ac3253bf2ed25af5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2d00300fae1442379e9308ac8411e53a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ae1137c21264f51a9808c25434dc7b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b459ba5db904da5aff0b12c7d366f27":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_45ccaa1cc2fe48b0894a4109e6046665","IPY_MODEL_5cab8ab59ac74e45b8a4e4e48309bfaf","IPY_MODEL_7aef3c768c5341ba97173b8b701edf95"],"layout":"IPY_MODEL_e4e0bc45241a48fe9b0f7534bb0a63c0"}},"45ccaa1cc2fe48b0894a4109e6046665":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_34c89c14235047738bf44e8a1313519a","placeholder":"â€‹","style":"IPY_MODEL_6dd573473c9e48b7b104eb3fb8cc6741","value":"Map:â€‡100%"}},"5cab8ab59ac74e45b8a4e4e48309bfaf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8583d542f0194016942c982f2076108e","max":3703,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9fd9c3e8e26f427caf457cd959f0fda1","value":3703}},"7aef3c768c5341ba97173b8b701edf95":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_747b489925054587a467aee7393ec4de","placeholder":"â€‹","style":"IPY_MODEL_21e599a03c404bcc9e1a03a1532eca39","value":"â€‡3703/3703â€‡[00:03&lt;00:00,â€‡1132.87â€‡examples/s]"}},"e4e0bc45241a48fe9b0f7534bb0a63c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34c89c14235047738bf44e8a1313519a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6dd573473c9e48b7b104eb3fb8cc6741":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8583d542f0194016942c982f2076108e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9fd9c3e8e26f427caf457cd959f0fda1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"747b489925054587a467aee7393ec4de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21e599a03c404bcc9e1a03a1532eca39":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9672146,"sourceType":"datasetVersion","datasetId":5893182}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install pip3-autoremove\n!pip-autoremove torch torchvision torchaudio -y\n!pip install --upgrade transformers\n!pip install unsloth","metadata":{"_uuid":"e7b755e0-3342-49fa-bbca-5ee0cfcf7e70","_cell_guid":"6bcfbd25-2de3-4c28-b959-e66d12021a32","collapsed":false,"id":"2eSvM9zX_2d3","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-20T06:50:57.576698Z","iopub.execute_input":"2024-10-20T06:50:57.577652Z","iopub.status.idle":"2024-10-20T06:54:35.337681Z","shell.execute_reply.started":"2024-10-20T06:50:57.577594Z","shell.execute_reply":"2024-10-20T06:54:35.336581Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import warnings\nimport torch\nfrom kaggle_secrets import UserSecretsClient\nimport os\n\nwarnings.filterwarnings(\"ignore\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Your device:\", device)\nuser_secrets = UserSecretsClient()\nos.environ[\"WANDB_API_KEY\"]=user_secrets.get_secret(\"WANDB_API_KEY\")\nos.environ[\"WANDB_PROJECT\"]=\"Text2Cypher\"","metadata":{"_uuid":"6647180a-392e-47cd-b55d-03495f99ec76","_cell_guid":"a9a9bbdb-cfd5-4e82-84fc-0c7c7b6f274c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-20T06:54:35.340178Z","iopub.execute_input":"2024-10-20T06:54:35.340908Z","iopub.status.idle":"2024-10-20T06:54:37.582175Z","shell.execute_reply.started":"2024-10-20T06:54:35.340862Z","shell.execute_reply":"2024-10-20T06:54:37.581172Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Your device: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"from unsloth import FastLanguageModel\n\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"_uuid":"d1d106f0-d99d-4633-945f-584b9384197b","_cell_guid":"8290bb6d-7fdf-48e6-9af1-f1b894c9247f","collapsed":false,"id":"QmUBVEnvCDJv","outputId":"056d28cc-3f86-4f08-f70e-b4058bf8e3b8","jupyter":{"outputs_hidden":false},"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-10-20T06:54:37.583692Z","iopub.execute_input":"2024-10-20T06:54:37.584208Z","iopub.status.idle":"2024-10-20T06:55:31.230271Z","shell.execute_reply.started":"2024-10-20T06:54:37.584160Z","shell.execute_reply":"2024-10-20T06:55:31.229110Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n==((====))==  Unsloth 2024.10.2: Fast Llama patching. Transformers = 4.44.2.\n   \\\\   /|    GPU: Tesla P100-PCIE-16GB. Max memory: 15.888 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 6.0. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post1. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b26951591514557b646b4caf054aa0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97d53eb946f64ab8805a30bf793be414"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ffe909dcaa1429f8d59cc315c4564f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6783d4e1e764c21bb5c80a9c307020c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/340 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"982fb7f37cee4ad794ad09dd0a9aa210"}},"metadata":{}},{"name":"stderr","text":"Unsloth: We fixed a gradient accumulation bug, but it seems like you don't have the latest transformers version!\nPlease update transformers via:\n`pip uninstall transformers -y && pip install --upgrade --no-cache-dir \"git+https://github.com/huggingface/transformers.git\"`\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We now add LoRA adapters so we only need to update 1 to 10% of all parameters!","metadata":{"_uuid":"0c45317a-66aa-43f0-a1e3-ea25ccadc251","_cell_guid":"6a3c4363-0fa8-40b0-a701-103821ec9e01","id":"SXd9bTZd1aaL","trusted":true}},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"_uuid":"e0541013-f227-4093-8e31-47ca9ec4541f","_cell_guid":"c1a001aa-7896-43b8-a3fb-b078844e35a8","collapsed":false,"id":"6bZsfBuZDeCL","outputId":"10350592-b856-4a6d-f587-8353977f8522","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-20T06:55:31.233072Z","iopub.execute_input":"2024-10-20T06:55:31.233474Z","iopub.status.idle":"2024-10-20T06:55:37.112282Z","shell.execute_reply.started":"2024-10-20T06:55:31.233425Z","shell.execute_reply":"2024-10-20T06:55:37.111258Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Unsloth 2024.10.2 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Data Prep\nWe now use the `Llama-3.1` format for conversation style finetunes. conversations like below:\n\n```\n<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nHello!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nHey there! How are you?<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nI'm great thanks!<|eot_id|>\n```","metadata":{"_uuid":"ad908349-c6c3-49cd-9c44-1bb18b9a1436","_cell_guid":"6a9f2228-fa41-4514-8e1b-bad8946ab80a","id":"vITh0KVJ10qX","trusted":true}},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\n\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"llama-3.1\",\n)\n\ndef formatting_prompts_func(examples):\n    convos = examples[\"conversations\"]\n    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n    return { \"text\" : texts, }\n\nfrom datasets import load_dataset\n\ntrainset = load_dataset('json', data_files='/kaggle/input/text-to-cypher/training_set.jsonl', split=\"train\")\nvalidationset = load_dataset('json', data_files='/kaggle/input/text-to-cypher/validation_set.jsonl', split=\"train\")","metadata":{"_uuid":"9d57be5b-cc54-4c9f-9881-bb91f36cc0ff","_cell_guid":"49edd11f-0e74-41e2-9d88-ce6706aebc48","collapsed":false,"id":"LjY75GoYUCB8","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-20T06:55:37.113620Z","iopub.execute_input":"2024-10-20T06:55:37.114008Z","iopub.status.idle":"2024-10-20T06:55:38.147180Z","shell.execute_reply.started":"2024-10-20T06:55:37.113965Z","shell.execute_reply":"2024-10-20T06:55:38.146252Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2716ea28bce4da7a725f92061f124d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b7a17ab7e0e42d3a453e0c63a667254"}},"metadata":{}}]},{"cell_type":"code","source":"trainset = trainset.map(formatting_prompts_func, batched = True,)\nvalidationset = validationset.map(formatting_prompts_func, batched = True,)","metadata":{"_uuid":"c67c3150-8a14-43fa-bf1f-db8613e6bb0c","_cell_guid":"0bea3436-aba4-4a2c-aaf5-ed618115cd05","collapsed":false,"id":"oPXzJZzHEgXe","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-20T06:55:38.148310Z","iopub.execute_input":"2024-10-20T06:55:38.148639Z","iopub.status.idle":"2024-10-20T06:55:39.808201Z","shell.execute_reply.started":"2024-10-20T06:55:38.148604Z","shell.execute_reply":"2024-10-20T06:55:39.807225Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3363 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c81c90c7cd54a959ec8e51021b95e44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/178 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b06603752c2b429e99cca736cbb15282"}},"metadata":{}}]},{"cell_type":"markdown","source":"We look at how the conversations are structured for item 5:","metadata":{"_uuid":"1c5269ce-5112-4eb9-986e-0819e9ab223b","_cell_guid":"188c0eed-9eb4-4efb-9d56-fb4e49811c53","id":"ndDUB23CGAC5","trusted":true}},{"cell_type":"code","source":"trainset[5][\"conversations\"]","metadata":{"_uuid":"ef02ef93-9737-4182-9cbc-70d1f7067d84","_cell_guid":"ce90edd2-648c-45a2-b194-3619eba2ed20","collapsed":false,"id":"gGFzmplrEy9I","outputId":"d087609d-c747-4626-a2f8-9aadf3c52c43","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-20T06:55:39.809714Z","iopub.execute_input":"2024-10-20T06:55:39.810111Z","iopub.status.idle":"2024-10-20T06:55:39.818825Z","shell.execute_reply.started":"2024-10-20T06:55:39.810066Z","shell.execute_reply":"2024-10-20T06:55:39.817765Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"[{'role': 'system',\n  'content': 'You are an experienced Cypher developer and English-speaking recruiter and a helpful assistant designed to output JSON.\\nConsider the Neo4j graph database with the following schema:\\n\\nNode properties are the following:\\n    Person {{ID: STRING, name: STRING, phone: STRING, dob: DATE, summary: STRING, location: STRING, portfolio: STRING}}\\n    Publication {{CATEGORY: STRING}}\\n    Certification {{CATEGORY: STRING}}\\n    Education {{NAME: STRING, Field: STRING}}\\n    WorkPlace {{NAME: STRING, domain_expertise: STRING, size: STRING}}\\n    Skills {{NAME: STRING, category: STRING}}\\n    Position {{NAME: STRING, description: STRING}}\\n\\nRelationship properties are the following:\\n    Achieve {{CONFERENCE: STRING, Date: DATE}}\\n    Study_at {{DEGREE: STRING, start_year: INT, end_year: INT, grade: STRING, achievement: STRING}}\\n    Worked_at {{ID: STRING, duration: INT, role: STRING, start_date: DATE, end_date: DATE, responsibilities: STRING, achievements: STRING}}\\n    Experience {{ID: STRING, duration: INT, responsibilities: STRING, achievements: STRING}}\\n    Relevant_to {{no properties listed}}\\n    Has {{no properties listed}}\\n\\nThe relationships are the following:\\n    (:Person)-[:Achieve]->(:Publication)\\n    (:Person)-[:Achieve]->(:Certification)\\n    (:Person)-[:Has]->(:Skills)\\n    (:Person)-[:Experience]->(:Position)\\n    (:Person)-[:Worked_at]->(:WorkPlace)\\n    (:Person)-[:Study_at]->(:Education)\\n    (:Certification)-[:Relevant_to]->(:Skills)\\n    (:Position)-[:Relevant_to]->(:Skills)\\n\\nWrite a Cypher query that would help you answer the following question:\\n'},\n {'role': 'user',\n  'content': 'What are the certifications of the Frontend Developer position?'},\n {'role': 'assistant',\n  'content': 'MATCH (pos:Position {NAME: \"Frontend Developer\"})-[:Relevant_to]->(s:Skills)-[:Relevant_to]->(c:Certification) RETURN pos.NAME AS PositionName, c.CATEGORY AS CertificationCategory'}]"},"metadata":{}}]},{"cell_type":"markdown","source":"And we see how the chat template transformed these conversations.","metadata":{"_uuid":"6f86f693-937a-4f82-ad59-c83bcee9bcb1","_cell_guid":"eab9dc2a-2486-477b-b29e-7ee1019b9ffc","id":"GfzTdMtvGE6w","trusted":true}},{"cell_type":"code","source":"print(trainset[5][\"text\"])","metadata":{"_uuid":"ab25403f-22de-47c9-8fc4-24d9e378b90a","_cell_guid":"79ed0d27-2bc9-48a7-b76a-eb18b2b5e5f5","collapsed":false,"id":"vhXv0xFMGNKE","outputId":"666cc2f2-441d-43bb-a32d-84943b5df501","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-20T06:55:39.820273Z","iopub.execute_input":"2024-10-20T06:55:39.820677Z","iopub.status.idle":"2024-10-20T06:55:39.846368Z","shell.execute_reply.started":"2024-10-20T06:55:39.820633Z","shell.execute_reply":"2024-10-20T06:55:39.845350Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 July 2024\n\nYou are an experienced Cypher developer and English-speaking recruiter and a helpful assistant designed to output JSON.\nConsider the Neo4j graph database with the following schema:\n\nNode properties are the following:\n    Person {{ID: STRING, name: STRING, phone: STRING, dob: DATE, summary: STRING, location: STRING, portfolio: STRING}}\n    Publication {{CATEGORY: STRING}}\n    Certification {{CATEGORY: STRING}}\n    Education {{NAME: STRING, Field: STRING}}\n    WorkPlace {{NAME: STRING, domain_expertise: STRING, size: STRING}}\n    Skills {{NAME: STRING, category: STRING}}\n    Position {{NAME: STRING, description: STRING}}\n\nRelationship properties are the following:\n    Achieve {{CONFERENCE: STRING, Date: DATE}}\n    Study_at {{DEGREE: STRING, start_year: INT, end_year: INT, grade: STRING, achievement: STRING}}\n    Worked_at {{ID: STRING, duration: INT, role: STRING, start_date: DATE, end_date: DATE, responsibilities: STRING, achievements: STRING}}\n    Experience {{ID: STRING, duration: INT, responsibilities: STRING, achievements: STRING}}\n    Relevant_to {{no properties listed}}\n    Has {{no properties listed}}\n\nThe relationships are the following:\n    (:Person)-[:Achieve]->(:Publication)\n    (:Person)-[:Achieve]->(:Certification)\n    (:Person)-[:Has]->(:Skills)\n    (:Person)-[:Experience]->(:Position)\n    (:Person)-[:Worked_at]->(:WorkPlace)\n    (:Person)-[:Study_at]->(:Education)\n    (:Certification)-[:Relevant_to]->(:Skills)\n    (:Position)-[:Relevant_to]->(:Skills)\n\nWrite a Cypher query that would help you answer the following question:\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nWhat are the certifications of the Frontend Developer position?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nMATCH (pos:Position {NAME: \"Frontend Developer\"})-[:Relevant_to]->(s:Skills)-[:Relevant_to]->(c:Certification) RETURN pos.NAME AS PositionName, c.CATEGORY AS CertificationCategory<|eot_id|>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name=\"Train\"></a>\n### Train the model\nNow let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!","metadata":{"_uuid":"eb147401-2d16-414d-ac4a-20e1a42db291","_cell_guid":"c2fb74d0-3ecd-4575-a588-8d1198b72f8d","id":"idAEIeSQ3xdS","trusted":true}},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments, DataCollatorForSeq2Seq\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = trainset,\n    eval_dataset = validationset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 8,\n        per_device_eval_batch_size = 8,\n        gradient_accumulation_steps = 4,\n        eval_accumulation_steps = 4,\n        warmup_steps = 10,\n        num_train_epochs = 2,\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 26,\n        eval_steps = 26,\n        eval_strategy=\"steps\",\n        logging_strategy=\"steps\",\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"cosine\",\n        seed = 3407,\n        output_dir = \"Llama3.1-8B-Instruct\",\n        report_to = \"wandb\"\n    ),\n)","metadata":{"_uuid":"a9c1c865-9ab9-4f2c-9dbb-e4104898dfab","_cell_guid":"ff565620-3b94-49d0-bdc7-0a4d094ef5c8","collapsed":false,"id":"95_Nn-89DhsL","outputId":"5c477d47-dfd6-4659-c686-10a5df3e32a5","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-20T06:55:39.847449Z","iopub.execute_input":"2024-10-20T06:55:39.847821Z","iopub.status.idle":"2024-10-20T06:55:45.051844Z","shell.execute_reply.started":"2024-10-20T06:55:39.847780Z","shell.execute_reply":"2024-10-20T06:55:45.050955Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/3363 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a637fcbae3df43908465441f7600589b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/178 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a5342fc39fc4a5d9596197166c9ffe2"}},"metadata":{}}]},{"cell_type":"markdown","source":"We also use Unsloth's train_on_responses_only method to only train on the assistant outputs and ignore the loss on the user's inputs.","metadata":{}},{"cell_type":"code","source":"from unsloth.chat_templates import train_on_responses_only\ntrainer = train_on_responses_only(\n    trainer,\n    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-20T06:55:45.055207Z","iopub.execute_input":"2024-10-20T06:55:45.055564Z","iopub.status.idle":"2024-10-20T06:55:47.112943Z","shell.execute_reply.started":"2024-10-20T06:55:45.055522Z","shell.execute_reply":"2024-10-20T06:55:47.112044Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3363 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cda8fdcdbf6463b8259c0264f947110"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/178 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83ad7d7a8bd0439885eccbf08f68269e"}},"metadata":{}}]},{"cell_type":"markdown","source":"We verify masking is actually done:","metadata":{}},{"cell_type":"code","source":"tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])","metadata":{"execution":{"iopub.status.busy":"2024-10-20T06:55:47.114316Z","iopub.execute_input":"2024-10-20T06:55:47.114744Z","iopub.status.idle":"2024-10-20T06:55:47.128355Z","shell.execute_reply.started":"2024-10-20T06:55:47.114689Z","shell.execute_reply":"2024-10-20T06:55:47.127425Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\nYou are an experienced Cypher developer and English-speaking recruiter and a helpful assistant designed to output JSON.\\nConsider the Neo4j graph database with the following schema:\\n\\nNode properties are the following:\\n    Person {{ID: STRING, name: STRING, phone: STRING, dob: DATE, summary: STRING, location: STRING, portfolio: STRING}}\\n    Publication {{CATEGORY: STRING}}\\n    Certification {{CATEGORY: STRING}}\\n    Education {{NAME: STRING, Field: STRING}}\\n    WorkPlace {{NAME: STRING, domain_expertise: STRING, size: STRING}}\\n    Skills {{NAME: STRING, category: STRING}}\\n    Position {{NAME: STRING, description: STRING}}\\n\\nRelationship properties are the following:\\n    Achieve {{CONFERENCE: STRING, Date: DATE}}\\n    Study_at {{DEGREE: STRING, start_year: INT, end_year: INT, grade: STRING, achievement: STRING}}\\n    Worked_at {{ID: STRING, duration: INT, role: STRING, start_date: DATE, end_date: DATE, responsibilities: STRING, achievements: STRING}}\\n    Experience {{ID: STRING, duration: INT, responsibilities: STRING, achievements: STRING}}\\n    Relevant_to {{no properties listed}}\\n    Has {{no properties listed}}\\n\\nThe relationships are the following:\\n    (:Person)-[:Achieve]->(:Publication)\\n    (:Person)-[:Achieve]->(:Certification)\\n    (:Person)-[:Has]->(:Skills)\\n    (:Person)-[:Experience]->(:Position)\\n    (:Person)-[:Worked_at]->(:WorkPlace)\\n    (:Person)-[:Study_at]->(:Education)\\n    (:Certification)-[:Relevant_to]->(:Skills)\\n    (:Position)-[:Relevant_to]->(:Skills)\\n\\nWrite a Cypher query that would help you answer the following question:\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat are the certifications of the Frontend Developer position?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nMATCH (pos:Position {NAME: \"Frontend Developer\"})-[:Relevant_to]->(s:Skills)-[:Relevant_to]->(c:Certification) RETURN pos.NAME AS PositionName, c.CATEGORY AS CertificationCategory<|eot_id|>'"},"metadata":{}}]},{"cell_type":"code","source":"space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\ntokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])","metadata":{"execution":{"iopub.status.busy":"2024-10-20T06:55:47.129724Z","iopub.execute_input":"2024-10-20T06:55:47.130044Z","iopub.status.idle":"2024-10-20T06:55:47.176907Z","shell.execute_reply.started":"2024-10-20T06:55:47.130012Z","shell.execute_reply":"2024-10-20T06:55:47.175971Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'                                                                                                                                                                                                                                                                                                                                                                                                                       \\n\\nMATCH (pos:Position {NAME: \"Frontend Developer\"})-[:Relevant_to]->(s:Skills)-[:Relevant_to]->(c:Certification) RETURN pos.NAME AS PositionName, c.CATEGORY AS CertificationCategory<|eot_id|>'"},"metadata":{}}]},{"cell_type":"markdown","source":"We can see the System and Instruction prompts are successfully masked!","metadata":{"_uuid":"fe0ba9d7-a031-450a-beeb-1a68cde5cdee","_cell_guid":"15c54306-e8fd-4a30-90e9-484fc704475a","id":"3enWUM0jV-jV","trusted":true}},{"cell_type":"code","source":"#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"_uuid":"13f478ae-98ec-4179-afff-7f17f22bc622","_cell_guid":"d1356e04-f3cc-4a07-9680-2cac2263a5d6","collapsed":false,"cellView":"form","id":"2ejIt2xSNKKp","outputId":"f4538e04-3652-4848-8788-26b93532e3b2","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-20T06:55:47.178024Z","iopub.execute_input":"2024-10-20T06:55:47.178300Z","iopub.status.idle":"2024-10-20T06:55:47.187203Z","shell.execute_reply.started":"2024-10-20T06:55:47.178270Z","shell.execute_reply":"2024-10-20T06:55:47.186384Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"GPU = Tesla P100-PCIE-16GB. Max memory = 15.888 GB.\n5.984 GB of memory reserved.\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"_uuid":"5f187a26-43ba-43d9-879c-b61d10572ac2","_cell_guid":"c208665c-c01b-4f86-95a2-a53a77b06145","collapsed":false,"id":"yqxqAZ7KJ4oL","outputId":"9067570a-ef20-423d-c235-0028fe31903f","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-20T06:55:47.188196Z","iopub.execute_input":"2024-10-20T06:55:47.188490Z","iopub.status.idle":"2024-10-20T13:24:18.586678Z","shell.execute_reply.started":"2024-10-20T06:55:47.188459Z","shell.execute_reply":"2024-10-20T13:24:18.585727Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 3,363 | Num Epochs = 2\nO^O/ \\_/ \\    Batch size per device = 8 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 32 | Total steps = 210\n \"-____-\"     Number of trainable parameters = 41,943,040\n","output_type":"stream"},{"name":"stdout","text":"**** Unsloth: Please use our fixed gradient_accumulation_steps by updating transformers and Unsloth!\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mngocnguyen14073\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111352387777755, max=1.0)â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92fbcdc7b90a4a1eb461ea4d5315d663"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241020_065552-unr2hdwg</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ngocnguyen14073/Text2Cypher/runs/unr2hdwg' target=\"_blank\">Llama3.1-8B-Instruct</a></strong> to <a href='https://wandb.ai/ngocnguyen14073/Text2Cypher' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ngocnguyen14073/Text2Cypher' target=\"_blank\">https://wandb.ai/ngocnguyen14073/Text2Cypher</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ngocnguyen14073/Text2Cypher/runs/unr2hdwg' target=\"_blank\">https://wandb.ai/ngocnguyen14073/Text2Cypher/runs/unr2hdwg</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [210/210 6:26:32, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>26</td>\n      <td>0.213500</td>\n      <td>0.065711</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>0.062200</td>\n      <td>0.045981</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>0.048200</td>\n      <td>0.041046</td>\n    </tr>\n    <tr>\n      <td>104</td>\n      <td>0.040700</td>\n      <td>0.035606</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.031300</td>\n      <td>0.033628</td>\n    </tr>\n    <tr>\n      <td>156</td>\n      <td>0.030800</td>\n      <td>0.029052</td>\n    </tr>\n    <tr>\n      <td>182</td>\n      <td>0.028700</td>\n      <td>0.027668</td>\n    </tr>\n    <tr>\n      <td>208</td>\n      <td>0.028900</td>\n      <td>0.027038</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"#@title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory         /max_memory*100, 3)\nlora_percentage = round(used_memory_for_lora/max_memory*100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"_uuid":"3346a94b-df83-4566-ae96-d41271283d7e","_cell_guid":"a05f7daa-ccd3-417b-b667-e66455e2b6fc","collapsed":false,"cellView":"form","id":"pCqnaKmlO1U9","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-20T13:24:18.588141Z","iopub.execute_input":"2024-10-20T13:24:18.588879Z","iopub.status.idle":"2024-10-20T13:24:18.598259Z","shell.execute_reply.started":"2024-10-20T13:24:18.588832Z","shell.execute_reply":"2024-10-20T13:24:18.597352Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"23307.7642 seconds used for training.\n388.46 minutes used for training.\nPeak reserved memory = 12.758 GB.\nPeak reserved memory for training = 6.774 GB.\nPeak reserved memory % of max memory = 80.3 %.\nPeak reserved memory for training % of max memory = 42.636 %.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save_pretrained(\"lora_model\") # Local saving\ntokenizer.save_pretrained(\"lora_model\")","metadata":{"_uuid":"a6943875-265b-4353-b389-e3f09dc4ff7b","_cell_guid":"7d04d3c3-7cc3-4c46-a0fd-88d09363c263","collapsed":false,"id":"FqfebeAdT073","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-20T13:24:18.599601Z","iopub.execute_input":"2024-10-20T13:24:18.600012Z","iopub.status.idle":"2024-10-20T13:24:19.870963Z","shell.execute_reply.started":"2024-10-20T13:24:18.599969Z","shell.execute_reply":"2024-10-20T13:24:19.869965Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"('lora_model/tokenizer_config.json',\n 'lora_model/special_tokens_map.json',\n 'lora_model/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"!zip -r model.zip /kaggle/working","metadata":{"_uuid":"3e9ba339-037f-4f9b-9660-49bb06b0db09","_cell_guid":"a22a285e-fdca-4315-bd28-97927a212610","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-10-20T13:33:11.681724Z","iopub.execute_input":"2024-10-20T13:33:11.682158Z","iopub.status.idle":"2024-10-20T13:33:36.015315Z","shell.execute_reply.started":"2024-10-20T13:33:11.682116Z","shell.execute_reply":"2024-10-20T13:33:36.014322Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"  adding: kaggle/working/ (stored 0%)\n  adding: kaggle/working/huggingface_tokenizers_cache/ (stored 0%)\n  adding: kaggle/working/huggingface_tokenizers_cache/.locks/ (stored 0%)\n  adding: kaggle/working/huggingface_tokenizers_cache/.locks/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/ (stored 0%)\n  adding: kaggle/working/huggingface_tokenizers_cache/.locks/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/165b36bc2293dda9a2fb3c0daf6577d9eba9df7a.lock (stored 0%)\n  adding: kaggle/working/huggingface_tokenizers_cache/.locks/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/d185c9e9b9d1e3f4e0c613d27efc412efdc3dde1.lock (stored 0%)\n  adding: kaggle/working/huggingface_tokenizers_cache/.locks/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/5cc5f00a5b203e90a27a3bd60d1ec393b07971e8.lock (stored 0%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/ (stored 0%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/refs/ (stored 0%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/refs/main (deflated 3%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/blobs/ (stored 0%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/blobs/d185c9e9b9d1e3f4e0c613d27efc412efdc3dde1 (deflated 94%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/blobs/165b36bc2293dda9a2fb3c0daf6577d9eba9df7a (deflated 61%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/blobs/5cc5f00a5b203e90a27a3bd60d1ec393b07971e8 (deflated 74%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/ (stored 0%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/5b0dd3039c312969e7950951486714bff26f0822/ (stored 0%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/5b0dd3039c312969e7950951486714bff26f0822/tokenizer_config.json (deflated 94%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/5b0dd3039c312969e7950951486714bff26f0822/tokenizer.json (deflated 74%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/snapshots/5b0dd3039c312969e7950951486714bff26f0822/special_tokens_map.json (deflated 61%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/.no_exist/ (stored 0%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/.no_exist/5b0dd3039c312969e7950951486714bff26f0822/ (stored 0%)\n  adding: kaggle/working/huggingface_tokenizers_cache/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/.no_exist/5b0dd3039c312969e7950951486714bff26f0822/added_tokens.json (stored 0%)\n  adding: kaggle/working/Llama3.1-8B-Instruct/ (stored 0%)\n  adding: kaggle/working/Llama3.1-8B-Instruct/checkpoint-210/ (stored 0%)\n  adding: kaggle/working/Llama3.1-8B-Instruct/checkpoint-210/training_args.bin (deflated 51%)\n  adding: kaggle/working/Llama3.1-8B-Instruct/checkpoint-210/trainer_state.json (deflated 75%)\n  adding: kaggle/working/Llama3.1-8B-Instruct/checkpoint-210/scheduler.pt (deflated 56%)\n  adding: kaggle/working/Llama3.1-8B-Instruct/checkpoint-210/README.md (deflated 66%)\n  adding: kaggle/working/Llama3.1-8B-Instruct/checkpoint-210/adapter_model.safetensors (deflated 8%)\n  adding: kaggle/working/Llama3.1-8B-Instruct/checkpoint-210/tokenizer_config.json (deflated 94%)\n  adding: kaggle/working/Llama3.1-8B-Instruct/checkpoint-210/adapter_config.json (deflated 54%)\n  adding: kaggle/working/Llama3.1-8B-Instruct/checkpoint-210/rng_state.pth (deflated 25%)\n  adding: kaggle/working/Llama3.1-8B-Instruct/checkpoint-210/tokenizer.json (deflated 74%)\n  adding: kaggle/working/Llama3.1-8B-Instruct/checkpoint-210/optimizer.pt (deflated 11%)\n  adding: kaggle/working/Llama3.1-8B-Instruct/checkpoint-210/special_tokens_map.json (deflated 71%)\n  adding: kaggle/working/lora_model/ (stored 0%)\n  adding: kaggle/working/lora_model/README.md (deflated 66%)\n  adding: kaggle/working/lora_model/adapter_model.safetensors (deflated 8%)\n  adding: kaggle/working/lora_model/tokenizer_config.json (deflated 94%)\n  adding: kaggle/working/lora_model/adapter_config.json (deflated 54%)\n  adding: kaggle/working/lora_model/tokenizer.json (deflated 74%)\n  adding: kaggle/working/lora_model/special_tokens_map.json (deflated 71%)\n  adding: kaggle/working/wandb/ (stored 0%)\n  adding: kaggle/working/wandb/run-20241020_065552-unr2hdwg/ (stored 0%)\n  adding: kaggle/working/wandb/run-20241020_065552-unr2hdwg/run-unr2hdwg.wandb (deflated 81%)\n  adding: kaggle/working/wandb/run-20241020_065552-unr2hdwg/files/ (stored 0%)\n  adding: kaggle/working/wandb/run-20241020_065552-unr2hdwg/files/wandb-metadata.json (deflated 47%)\n  adding: kaggle/working/wandb/run-20241020_065552-unr2hdwg/files/output.log (deflated 86%)\n  adding: kaggle/working/wandb/run-20241020_065552-unr2hdwg/files/requirements.txt (deflated 56%)\n  adding: kaggle/working/wandb/run-20241020_065552-unr2hdwg/logs/ (stored 0%)\n  adding: kaggle/working/wandb/run-20241020_065552-unr2hdwg/logs/debug-internal.log (deflated 71%)\n  adding: kaggle/working/wandb/run-20241020_065552-unr2hdwg/logs/debug.log (deflated 69%)\n  adding: kaggle/working/wandb/run-20241020_065552-unr2hdwg/logs/debug-core.log (deflated 58%)\n  adding: kaggle/working/wandb/run-20241020_065552-unr2hdwg/tmp/ (stored 0%)\n  adding: kaggle/working/wandb/run-20241020_065552-unr2hdwg/tmp/code/ (stored 0%)\n  adding: kaggle/working/wandb/debug-internal.log (deflated 71%)\n  adding: kaggle/working/wandb/debug.log (deflated 69%)\n  adding: kaggle/working/wandb/latest-run/ (stored 0%)\n  adding: kaggle/working/wandb/latest-run/run-unr2hdwg.wandb (deflated 81%)\n  adding: kaggle/working/wandb/latest-run/files/ (stored 0%)\n  adding: kaggle/working/wandb/latest-run/files/wandb-metadata.json (deflated 47%)\n  adding: kaggle/working/wandb/latest-run/files/output.log (deflated 86%)\n  adding: kaggle/working/wandb/latest-run/files/requirements.txt (deflated 56%)\n  adding: kaggle/working/wandb/latest-run/logs/ (stored 0%)\n  adding: kaggle/working/wandb/latest-run/logs/debug-internal.log (deflated 71%)\n  adding: kaggle/working/wandb/latest-run/logs/debug.log (deflated 69%)\n  adding: kaggle/working/wandb/latest-run/logs/debug-core.log (deflated 58%)\n  adding: kaggle/working/wandb/latest-run/tmp/ (stored 0%)\n  adding: kaggle/working/wandb/latest-run/tmp/code/ (stored 0%)\n  adding: kaggle/working/.virtual_documents/ (stored 0%)\n","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'model.zip')","metadata":{"execution":{"iopub.status.busy":"2024-10-20T13:34:09.091169Z","iopub.execute_input":"2024-10-20T13:34:09.091595Z","iopub.status.idle":"2024-10-20T13:34:09.099198Z","shell.execute_reply.started":"2024-10-20T13:34:09.091556Z","shell.execute_reply":"2024-10-20T13:34:09.098293Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/model.zip","text/html":"<a href='model.zip' target='_blank'>model.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}