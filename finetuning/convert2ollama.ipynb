{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EgfhaNtTR9N"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install jsonlines\n",
        "!pip install evaluate\n",
        "!pip install rouge_score\n",
        "!pip install bert_score\n",
        "!pip install unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZVQXrriUOlU",
        "outputId": "736be81c-5351-4cc2-8435-2c0304504a59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/results.zip\n",
            " extracting: huggingface_tokenizers_cache/.locks/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/165b36bc2293dda9a2fb3c0daf6577d9eba9df7a.lock  \n",
            " extracting: huggingface_tokenizers_cache/.locks/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/5cc5f00a5b203e90a27a3bd60d1ec393b07971e8.lock  \n",
            " extracting: huggingface_tokenizers_cache/.locks/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/d185c9e9b9d1e3f4e0c613d27efc412efdc3dde1.lock  \n",
            " extracting: huggingface_tokenizers_cache/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/.no_exist/5b0dd3039c312969e7950951486714bff26f0822/added_tokens.json  \n",
            "  inflating: huggingface_tokenizers_cache/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/blobs/165b36bc2293dda9a2fb3c0daf6577d9eba9df7a  \n",
            "  inflating: huggingface_tokenizers_cache/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/blobs/5cc5f00a5b203e90a27a3bd60d1ec393b07971e8  \n",
            "  inflating: huggingface_tokenizers_cache/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/blobs/d185c9e9b9d1e3f4e0c613d27efc412efdc3dde1  \n",
            "  inflating: huggingface_tokenizers_cache/models--unsloth--meta-llama-3.1-8b-instruct-bnb-4bit/refs/main  \n",
            "  inflating: lora_model/README.md    \n",
            "  inflating: lora_model/adapter_config.json  \n",
            "  inflating: lora_model/adapter_model.safetensors  \n",
            "  inflating: lora_model/special_tokens_map.json  \n",
            "  inflating: lora_model/tokenizer.json  \n",
            "  inflating: lora_model/tokenizer_config.json  \n",
            "  inflating: outputs/checkpoint-104/README.md  \n",
            "  inflating: outputs/checkpoint-104/adapter_config.json  \n",
            "  inflating: outputs/checkpoint-104/adapter_model.safetensors  \n",
            "  inflating: outputs/checkpoint-104/optimizer.pt  \n",
            "  inflating: outputs/checkpoint-104/rng_state.pth  \n",
            "  inflating: outputs/checkpoint-104/scheduler.pt  \n",
            "  inflating: outputs/checkpoint-104/special_tokens_map.json  \n",
            "  inflating: outputs/checkpoint-104/tokenizer.json  \n",
            "  inflating: outputs/checkpoint-104/tokenizer_config.json  \n",
            "  inflating: outputs/checkpoint-104/trainer_state.json  \n",
            "  inflating: outputs/checkpoint-104/training_args.bin  \n",
            "  inflating: wandb/run-20241018_023357-qa62nvv4/files/output.log  \n",
            "  inflating: wandb/run-20241018_023357-qa62nvv4/files/requirements.txt  \n",
            "  inflating: wandb/run-20241018_023357-qa62nvv4/files/wandb-metadata.json  \n",
            "  inflating: wandb/run-20241018_023357-qa62nvv4/logs/debug-internal.log  \n",
            "  inflating: wandb/run-20241018_023357-qa62nvv4/logs/debug.log  \n",
            "  inflating: wandb/run-20241018_023357-qa62nvv4/run-qa62nvv4.wandb  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/results.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKX_XKs_BNZR",
        "outputId": "bba84fdd-440e-4fbe-c8a0-343f89e49e94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "==((====))==  Unsloth 2024.10.2: Fast Llama patching. Transformers = 4.44.2.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: We fixed a gradient accumulation bug, but it seems like you don't have the latest transformers version!\n",
            "Please update transformers via:\n",
            "`pip uninstall transformers -y && pip install --upgrade --no-cache-dir \"git+https://github.com/huggingface/transformers.git\"`\n",
            "Unsloth 2024.10.2 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orrApmGB77vH"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDp0zNpwe6U_"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in `llama.cpp`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YO5PBWLag13w"
      },
      "outputs": [],
      "source": [
        "print(tokenizer._ollama_modelfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyzyfLm1g_hp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
