{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09adoXhWsj-V"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install jsonlines\n",
    "!pip install unsloth==2024.11.7\n",
    "!unzip /content/model.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "NZr3CbR8sqj0",
    "outputId": "fd61ef5a-9ef2-42d7-dc7e-2a5cf54cef23"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "fined_model, fined_tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "haMSZH_csvLz"
   },
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_0f0wGXszer"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "fined_tokenizer = get_chat_template(\n",
    "    fined_tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9052P6a5s1pT"
   },
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "from tqdm import tqdm\n",
    "\n",
    "def inferences(model):\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    with jsonlines.open(\"/content/validation_set.jsonl\", \"r\") as reader:\n",
    "        for line in tqdm(list(reader)):\n",
    "            inputs = tokenizer.apply_chat_template(\n",
    "                line[\"conversations\"][:2],\n",
    "                tokenize = True,\n",
    "                add_generation_prompt = True, # Must add for generation\n",
    "                return_tensors = \"pt\",\n",
    "            ).to(\"cuda\")\n",
    "            inputs_length = inputs.shape[1]\n",
    "\n",
    "            outputs = model.generate(input_ids = inputs, max_new_tokens = 2056,\n",
    "                      use_cache = True, temperature = 0.0, do_sample=False)\n",
    "\n",
    "\n",
    "            results = tokenizer.batch_decode(outputs[:, inputs_length:])[0].removesuffix(\"<|eot_id|>\")\n",
    "            predictions.append(results)\n",
    "            references.append(line[\"conversations\"][-1][\"content\"])\n",
    "\n",
    "    return predictions, references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0y4wPEXs4VE"
   },
   "outputs": [],
   "source": [
    "fined_predictions, fined_references = inferences(fined_model)\n",
    "predictions, references = inferences(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSwAuBpns8B7"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the variables to a pickle file\n",
    "with open('predictions_references.pkl', 'wb') as file:\n",
    "    pickle.dump({\n",
    "        'fined_predictions': fined_predictions,\n",
    "        'fined_references': fined_references,\n",
    "        'predictions': predictions,\n",
    "        'references': references\n",
    "    }, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdAAWuHMtRNb"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Example file path (replace this with the path to your file)\n",
    "file_path = '/content/predictions_references.pkl'  # or '/content/predictions_references.json'\n",
    "\n",
    "# Trigger download\n",
    "files.download(file_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
